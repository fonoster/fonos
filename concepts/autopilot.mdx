---
title: 'Autopilot'
description: 'Voice applications powered by LLMs.'
---

import WipDocsWarning from '/snippets/wip-docs-warning.mdx';

<Warning>
  The Autopilot is currently in overview mode and only available via the SDK, and the KnowledgeBase features have been disabled.
</Warning>

Fonoster's Autopilot is a component within the platform that allows you to create powerful conversational experiences. It is built on top of Fonoster [Programmable Voice](./programmable-voice) and uses the latest advances in large language models (LLMs) to provide a natural and engaging experience.

## Overview

The following is an example of how to create an Autopilot application using the SDK:

```javascript index.js {7,20-39}
const SDK = require("@fonoster/sdk");

const client = new SDK.Client({ accessKeyId: "WO000000-0000-0000-0000-000000000000" });

const appConfig = {
  name: "Dr. Green's AI Assistant",
  type: "AUTOPILOT",
  speechToText: {
    productRef: "stt.deepgram",
    config: {
      languageCode: "en-US"
    }
  },
  textToSpeech: {
    productRef: "tts.deepgram",
    config: {
      voice: "aura-asteria-en"
    }
  },
  intelligence: {
    productRef: "llm.groq",
    config: {
      conversationSettings: {
        firstMessage: "Hello, this is Olivia from Dr. Green's Family Medicine. How can I assist you today?",
        systemPrompt: "You are a Customer Service Representative. You are here to help the caller with their needs.",
        goodbyeMessage: "Goodbye, have a great day!",
        systemErrorMessage: "I'm sorry, I didn't understand that. Can you please repeat it?",
        idleOptions: {
          "message": "Are you still there?"
        }
      },
      languageModel: {
        provider: "groq",
        model: "llama-3.3-70b-specdec",
        maxTokens: 250,
        temperature: 0.7
      }
    }
  }
}

client.loginWithApiKey("AP0eerv2g7qow3e950k7twu4rvydcunq3k", "fNc...")
  .then(async() => new SDK.Applications(client).createApplication(appConfig))
  .catch(console.error);
```

## General configuration

The Autopilot configuration is divided into a general section and three sub-sections: _speechToText_, _textToSpeech_, and _intelligence_.

The general section contains _name_, _type_, and _endpoint_ properties.

The _name_ property is the name of the Autopilot application. The _type_ property is the type of the application, which should always be set to `AUTOPILOT`. The _endpoint_ is an optional property allowing you to specify the endpoint for self-hosted Autopilots.

## Speech settings

Autopilot applications support a variety of speech-to-text and text-to-speech vendors. The _speechToText_ and _textToSpeech_ objects allow you to define the speech-to-text and text-to-speech engines to use.

You can mix and match vendors to suit your needs. For example, you can use _Deepgram_ for speech-to-text and Google for text-to-speech. Please check the [Speech Vendors](./speech-vendors) section for more information on configuring speech-to-text and text-to-speech.

## Conversational settings

The _conversationSettings_ object allows you to define the Autopilot's conversational behavior. The conversation settings are independent of the language model used.

The following is a list of the supported settings:

| Setting                     | Description                                                                                                               |
| --------------------------- | ------------------------------------------------------------------------------------------------------------------------- |
| firstMessage                | The first message the Autopilot will say when the conversation starts                                                     |
| systemTemplate              | A template that describes the role of the Autopilot. This is used to set the context of the conversation                  |
| systemErrorMessage          | The message the Autopilot will say when an error occurs                                                                   |
| maxSessionDuration          | The maximum time in milliseconds the conversation can last. Default to `30 x 60 x 1000 ms` (30 minutes)                   |
| maxSpeechWaitTimeout        | The maximum time in milliseconds to wait for the caller before returning the speech-to-text result. Default to `5000 ms` |
| initialDtmf                 | A DTMF to play when the conversation starts                                                                               |
| transferOptions             | The options to transfer the call to a live agent                                                                          |
| transferOptions.phoneNumber | The phone number to transfer the call to                                                                                  |
| transferOptions.message     | The message to play before transferring the call                                                                          |
| transferOptions.timeout     | The time in milliseconds to wait before hanging up the call if the transfer is incomplete. Default to `30000 ms`       |
| idleOptions                 | The options to handle idle time during the conversation                                                                   |
| idleOptions.message         | The message to play after the idle time is reached                                                                        |
| idleOptions.timeout         | The time in milliseconds to wait before playing the idle message. Defaults to `10000 ms`                                  |
| idleOptions.maxTimeoutCount | The maximum number of times the idle message will be played before hanging up the call                                    |
| vad                         | The voice activity detection settings                                                                                     |
| vad.activationThreshold     | The activation threshold for the voice activity detection. Default to `0.3`                                               |
| vad.deactivationThreshold   | The deactivation threshold for the voice activity detection. Default to `0.25`                                            |
| vad.debounceFrames          | The number of frames to debounce the voice activity detection. Default to `3`                                             |

A few noteworthy settings include the *maxSpeechWaitTimeout*, *initialDtmf*, *idleOptions*, and *vad*.

### Max Speech Wait Timeout

The _maxSpeechWaitTimeout_ property allows you to specify the maximum time in milliseconds to wait for the caller before returning the speech-to-text result. If the caller does not speak within the specified time, the speech-to-text engine will return the result.

A value that is too low may result in the speech-to-text engine returning the result before the caller finishes speaking. A value that is too high may result in the speech-to-text engine waiting too long for the caller to speak.

### Initial DTMF

Sometimes, users will use call forwarding to reach the number in Fonoster. Some telephony service providers require a Dual-tone multi-frequency (DTMF) to be played before connecting the call. The _initialDtmf_ property allows you to specify a DTMF to play when the session starts.

### Voice Activity Detection (VAD)

The *vad* object allows you to configure the voice activity detection settings. Voice activity detection is used to detect when the caller is speaking and when they are not speaking.

The _vad_ object has the _activationThreshold_, _deactivationThreshold_, _debounceFrames_ properties. The _actionThreshold_ property is the activation threshold for voice activity detection. The _deactivationThreshold_ property is the deactivation threshold for voice activity detection. The _debounceFrames_ property is the number of frames to debounce the voice activity detection.

A lower activation threshold will make the detection more sensitive to the caller's speech. A higher activation threshold will make detecting voice activity less sensitive to the caller's speech.

A lower deactivation threshold will result in more aggressive voice activity detection deactivation. A higher deactivation threshold will result in less aggressive voice activity detection deactivation.

The _debounceFrames_ parameter introduces a delay mechanism that ensures that transitions between "speech" and "non-speech" states are stable and not too sensitive to small fluctuations in the input audio signal. Here's how it works:

By requiring multiple consecutive frames (debounceFrames) to confirm speech or non-speech, the system filters out short bursts of noise or brief gaps in speech that might otherwise cause erratic state changes.

## Langue model configuration

The _languageModel_ object allows you to define the language model the Autopilot uses. The language model is responsible for generating responses to the user's input.

The following is a list of the supported settings:

| Setting       | Description                                                                                     |
| ------------- | ----------------------------------------------------------------------------------------------- |
| provider      | The provider of the language model. Supported providers are `openai`, `groq`, and `ollama`      |
| model         | The model to use. The available models depend on the provider                                   |
| maxTokens     | The maximum number of tokens the language model can generate in a single response               |
| temperature   | The randomness of the language model. A higher temperature will result in more random responses |
| knowledgeBase | A list of knowledge bases to use for the language model                                         |
| tools         | A list of tools to use for the language model                                                   |

### LLM providers and models

The Autopilot supports multiple language model providers. The following is a list of the supported providers:

| Provider | Description                                                | Supported models                                                           |
| -------- | ---------------------------------------------------------- | -------------------------------------------------------------------------- |
| OpenAI   | OpenAI provides various GPT models for conversational AI   | `gpt-4o`, `gpt-4o-mini`, `gpt-3.5-turbo`, `gpt-4-turbo`                    |
| Groq     | Groq offers high-performance AI models optimized for speed | `llama-3.1-8b-instant`, `llama-3.3-70b-specdec`, `llama-3.3-70b-versatile` |
| Ollama   | Self-hosted Ollama models                                  | `llama3-groq-tool-use`                                                     |

<Warning>
  We have noticed that Groq models, particularly `llama-3.3-70b-versatile`, often require greater prompting specificity for effective tool usage. We will share best practices to ensure more consistent behavior as we gain more insights
</Warning>

### Knowledge bases

Coming soon...

### Tools

Fonoster's Autopilot allows you to use tools to enhance the conversational experience. Tools are used to perform specific actions during the conversation.

### Built-in tools

The following is a list of built-in tools available for an agent:

| Tool     | Description                                  |
| -------- | -------------------------------------------- |
| hangup   | A tool to end the conversation               |
| transfer | A tool to transfer the call to a live agent  |
| hold     | A tool to put the call on hold (Coming soon) |

### Custom tools

You can add custom tools to the language model by adding an entry to the `tools` array. The following is an example of how to add a custom tool:

Custom tools are governed by the [tool schema](https://github.com/fonoster/fonoster/blob/main/mods/common/src/assistants/tools/toolSchema.ts).

An custom tool to get available appointment times would look as follows:

```json
{
  "name": "getAvailableTimes",
  "description": "Get available appointment times for a specific date.",
  "requestStartMessage": "I'm looking for available appointment times for the date you provided.",
  "parameters": {
    "type": "object",
    "properties": {
      "date": {
        "type": "string",
        "format": "date"
      }
    },
    "required": [
      "date"
    ]
  },
  "operation": {
    "type": "get",
    "url": "https://api.example.com/appointment-times",
    "headers": {
      "x-api-key": "your-api-key"
    }
  }
}
```

<Tip>
  Use `operation.type` "post" for POST requests. If you want the Autopilot to wait for POST requests to complete, set `operation.waitForResponse` to `true`. For "get" requests, the Autopilot will wait for the response by default.
</Tip>